# -*- coding: utf-8 -*-
"""cluster_proto_resnet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yufbl0AnWDtJLTqQbnhpqf0PhpZrdW2v

# Data Load
"""

import keras
import numpy as np
import tensorflow as tf
import os
import glob
import pickle
from keras.datasets import cifar10
from sklearn.model_selection import ParameterGrid

def get_dataset(ds_name, normalize,ratio):
                
        x_train, x_val, x_test, y_train, y_val, y_test = get_cifar10_proto(ratio)
        normalize = True

        if normalize:
            x_train, x_val, x_test = normalize_data(ds_name, x_train, x_val, x_test)
    
        return x_train, x_val, x_test, y_train, y_val, y_test  

def train_val_test_splitter(X, y, ratio, random_state=999):
      x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=ratio, random_state=999)
      x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=ratio/(1-ratio), random_state=999)
    
      return x_train, x_val, x_test, y_train, y_val, y_test


def normalize_data(ds_name, x_train, x_val, x_test):
      if ds_name == 'cifar10' or ds_name == 'cifar10_proto':
            x_train = x_train/255
            x_val = x_val/255
            x_test = x_test/255
      return x_train, x_val, x_test

def get_cifar10_proto(ratio): 
      (x_train, y_train), (x_test, y_test) = cifar10.load_data()
      X = np.concatenate((x_train, x_test), axis=0)
      y = np.concatenate((y_train, y_test), axis=0)
      
      x_train = np.zeros([10, int(len(X)*(1-2*ratio)/10),32,32,3], dtype=np.float32)
      x_val = np.zeros([10, int(len(X)*(ratio)/10),32,32,3], dtype=np.float32)
      x_test = np.zeros([10, int(len(X)*(ratio)/10),32,32,3], dtype=np.float32)

      for cl in np.sort(np.unique(y)):
          x_train[cl] = X[np.where(y.T[0]==cl)[0][:int(len(X)*(1-2*ratio)/10)]]
          x_val[cl] = X[np.where(y.T[0]==cl)[0][int(len(X)*(1-2*ratio)/10):int(len(X)*(1-ratio)/10)]]
          x_test[cl] = X[np.where(y.T[0]==cl)[0][int(len(X)*(1-ratio)/10):]]
                     
      y_train = [i for i in range(10)]
      y_val = [i for i in range(10)]
      y_test = [i for i in range(10)]
               
      return x_train, x_val, x_test, y_train, y_val, y_test

#Run this to get train-val-test sets (For prototypical: cifar10_proto, omniglot_proto)
x_train, x_val, x_test, y_train, y_val, y_test = get_dataset('cifar10_proto',False,0.20)

x_train.shape, x_val.shape, x_test.shape
#y_train.shape, y_val.shape, y_test.shape
#len(y_train), len(y_val), len(y_test)


def percentage_splitter(train, val, yt ,yv ,merging ,random_selection ,ratio):
    
    if merging == True:

        if len(train.shape)==5:   #Prototypical
            train = np.concatenate((train, val), axis=1)
            if random_selection == True:
                train = shuffle(train, random_state=999)
                train2 = train[:,:int(np.ceil(train.shape[1]*ratio)),:,:,:]
            else:
                train2 = train[:,:int(np.ceil(train.shape[1]*ratio)),:,:,:]
            yt2=yt
            
        elif len(train.shape)==4: #Our Model
            train = np.concatenate((train, val), axis=0)
            yt = np.concatenate((yt, yv), axis=0)
            if random_selection == True:
                print("burasi")
                train, yt = shuffle(train, yt, random_state=999)
                train2 = train[:int(np.ceil(train.shape[0]*ratio)),:,:,:]
                yt2 = yt[:int(np.ceil(yt.shape[0]*ratio)),:]
            else:
                train2 = train[:int(np.ceil(train.shape[0]*ratio)),:,:,:]         
                yt2 = yt[:int(np.ceil(yt.shape[0]*ratio)),:]
            
    else:
        if len(train.shape)==5:   #Prototypical
            if random_selection == True:
                train = shuffle(train, random_state=999)
                train2 = train[:,:int(np.ceil(train.shape[1]*ratio)),:,:,:]
            else:
                train2 = train[:,:int(np.ceil(train.shape[1]*ratio)),:,:,:]
            yt2=yt
        elif len(train.shape)==4:  #Our Model
            if random_selection == True:
                train, yt = shuffle(train, yt, random_state=999)
                train2 = train[:int(np.ceil(train.shape[0]*ratio)),:,:,:]
                yt2 = yt[:int(np.ceil(yt.shape[0]*ratio)),:]
            else:
                train2 = train[:int(np.ceil(train.shape[0]*ratio)),:,:,:]
                yt = np.concatenate((yt, yv), axis=0)
                yt2 = yt[:int(np.ceil(yt.shape[0]*ratio)),:]
    return train2 , yt2


#Merging: merges train and validation
#Random_selection: makes train data shuffle before split so it would select different instances 
#Ratio: selects instances with the given percentage [0-1]
x_train2, y_train2 = percentage_splitter(x_train, x_val, y_train, y_val, merging=True, random_selection=False, ratio=0.5) 
x_train=x_train2; y_train=y_train2
x_train.shape, x_val.shape, x_test.shape
#y_train.shape, y_val.shape, y_test.shape

#For omniglot-prototypical
train_dataset = x_train
train_classes = y_train
val_dataset = x_val
val_classes = y_val
test_dataset = x_test
test_classes = y_test
n_classes = len(train_classes)
n_val_classes = len(val_classes)
n_test_classes = len(test_classes)

"""Constants and hyperparameters used by the cifar10 program. """

# Constants describing the CIFAR-10 data set.
IMAGE_SIZE = 32                     # Width and height of CIFAR-10 images. 
CHANNELS = 3                        # Number of color channels (RGB). 
NUM_CLASSES = 10                    # Number of CIFAR-10 classes. 
NUM_TRAIN_CIFAR10 = 50000           # Number of CIFAR-10 training instances. 
NUM_TRAIN_EXAMPLES = 50000          # Number of CIFAR-10 test instances. 
NUM_TEST_EXAMPLES = 10000           # Number of instances to use for training. 

# Constants describing the training process.
BATCH_SIZE = 128                    # Batch size. 
LR_BOUNDARIES = [400, 32000, 48000] # Learning rate boundaries.
LR_VALUES = [0.01, 0.1, 0.01, 0.001]# Learning rates.
MOMENTUM = 0.9                      # Momentum. 
TRAIN_STEPS = 65000                 # Number of steps to run. 
LOG_FREQUENCY = 1000                # How often to log results.

# Network hyperparameters
BN_MOMENTUM = 0.9                   # Decay rate for batch normalization.
SHORTCUT_L2_SCALE = 0.0001          # Regularization for the skip connections. 
DEPTH = 3                           # Residual units per stack. 
WIDEN_FACTOR = 1                    # Scale up the number of feature maps.

# Constants describing the input pipeline. 
SHUFFLE_BUFFER = NUM_TRAIN_EXAMPLES # Buffer size for the shuffled dataset.
NUM_THREADS = 6                     # Number of threads for image processing. 
OUTPUT_BUFFER_SIZE = BATCH_SIZE*2   # Buffer size for processed images. 
TRAIN_OUTPUT_BUFFER = SHUFFLE_BUFFER//BATCH_SIZE # Train buffer size. 
VALIDATION_OUTPUT_BUFFER = 6        # Buffer size for validation dataset.

print(train_dataset.shape)
print(len(train_classes));print(train_classes)
print(val_dataset.shape)
print(len(val_classes));print(val_classes)
print(test_dataset.shape)
print(len(test_classes));print(test_classes)

"""# Main Code"""

def _residual_block(inputs, filters, initializer, training, stride=1, first=False):
    assert(stride==1 or stride==2)
    bn1 = tf.keras.layers.BatchNormalization(
        momentum=BN_MOMENTUM, 
        fused=True, 
        scale=False)(inputs, training=training)
    relu1 = tf.nn.relu(bn1)
    conv1 = tf.keras.layers.Conv2D( 
        filters=filters, 
        kernel_size=3, 
        strides=stride, 
        padding='same', 
        kernel_initializer=initializer, 
        use_bias=False)(relu1)
    
    bn2 = tf.keras.layers.BatchNormalization(
        momentum=BN_MOMENTUM, 
        fused=True, 
        scale=False)(conv1, training=training)
    relu2 = tf.nn.relu(bn2)
    conv2 = tf.keras.layers.Conv2D(
        filters=filters, 
        kernel_size=3, 
        padding='same', 
        kernel_initializer=initializer, 
        use_bias=False)(relu2)
    
    shortcut = inputs
    if first:
        shortcut = relu1
    if stride==2:
        shortcut = tf.keras.layers.AveragePooling2D(2, 2)(shortcut)
        pad = tf.zeros(tf.shape(shortcut), tf.float32)
        shortcut = tf.concat([shortcut, pad], axis=3)
    
    return tf.add(shortcut, conv2)

def inference(images, training):
    """Build a ResNet for CIFAR-10.
    Args:
        images: Input for the network. 
            Tensor with shape (batches, height=32, width=32, channels=3). 
        training: Boolean tensor that specifies if the network is
            being trained. This is necessary for batch normalization. 
    Returns:
        A tensor that evaluates scores for the CIFAR-10 classes. 
    """
    initializer = tf.contrib.layers.variance_scaling_initializer()

    
    # Construct the first convolutional layer with no activation. 
    conv1 = tf.keras.layers.Conv2D(
        filters=16 * WIDEN_FACTOR, 
        kernel_size=3, 
        padding='same', 
        kernel_initializer = initializer,
        use_bias=False)(images)
    
    # Stack n residual blocks with 16*WIDEN_FACTOR feature maps sized 32x32. 

    block = conv1
    filters = 16 * WIDEN_FACTOR
    block = _residual_block(block, filters, initializer, training, first=True)
    for i in range(2, DEPTH+1):
            with tf.variable_scope('block{}'.format(i)):
                block = _residual_block(block, filters, initializer, training)
        
    # Stack n residual blocks with 32*WIDEN_FACTOR feature maps sized 16x16. 

    filters *= 2
    block = _residual_block(block, filters, initializer, training, stride=2)
    for i in range(2, DEPTH+1):
          block = _residual_block(block, filters, initializer, training)
        
    # Stack n residual blocks with 64*WIDEN_FACTOR feature maps sized 8x8. 
    filters *= 2
    block = _residual_block(block, filters, initializer, training, stride=2)
    for i in range(2, DEPTH+1):
          block = _residual_block(block, filters, initializer, training)
                
    # Batch Normalization and Rectified Linear Unit layers. 
    bn_final = tf.keras.layers.BatchNormalization(
        momentum=BN_MOMENTUM, 
        fused=True, 
        scale=False)(block, training=training)
    relu_final = tf.nn.relu(bn_final)

    # Global Average Pooling and Classifier. 
    global_average_pool = tf.keras.layers.AveragePooling2D(
        pool_size=(8, 8),
        strides=1, 
        name='global_average_pooling')(relu_final)
    global_average_flat = tf.reshape(
        global_average_pool, 
        [-1, filters])
    
    '''K = kernel(keys, global_average_flat)
    KV =  tf.matmul(tf.transpose(K), V)
    KV_ = tf.diag(tf.reshape(tf.reciprocal( tf.matmul(KV,tf.ones((n_output,1)))) , [-1]))
    logits = tf.matmul(KV_, KV)'''

    logits = tf.keras.layers.Dense(
        units=64, 
        kernel_initializer = initializer)(global_average_flat)
    print("resnet")
    return logits

def inference2(images, training):
    """Build a ResNet for CIFAR-10.
    Args:
        images: Input for the network. 
            Tensor with shape (batches, height=32, width=32, channels=3). 
        training: Boolean tensor that specifies if the network is
            being trained. This is necessary for batch normalization. 
    Returns:
        A tensor that evaluates scores for the CIFAR-10 classes. 
    """
    initializer = tf.contrib.layers.variance_scaling_initializer()

    
    # Construct the first convolutional layer with no activation. 
    conv1 = tf.keras.layers.Conv2D(
        filters=16 * WIDEN_FACTOR, 
        kernel_size=3, 
        padding='same', 
        kernel_initializer = initializer,
        use_bias=False)(images)
    
    # Stack n residual blocks with 16*WIDEN_FACTOR feature maps sized 32x32. 

    block = conv1
    filters = 16 * WIDEN_FACTOR
    block = _residual_block(block, filters, initializer, training, first=True)
    for i in range(2, DEPTH+1):
                block = _residual_block(block, filters, initializer, training)
        
    # Stack n residual blocks with 32*WIDEN_FACTOR feature maps sized 16x16. 
    with tf.variable_scope('stack2'):
        filters *= 2
        with tf.variable_scope('block1'):
            block = _residual_block(block, filters, initializer, training, stride=2)
        for i in range(2, DEPTH+1):
            with tf.variable_scope('block{}'.format(i)):
                block = _residual_block(block, filters, initializer, training)
        
    # Stack n residual blocks with 64*WIDEN_FACTOR feature maps sized 8x8. 
    with tf.variable_scope('stack3'):
        filters *= 2
        with tf.variable_scope('block1'):
            block = _residual_block(block, filters, initializer, training, stride=2)
        for i in range(2, DEPTH+1):
            with tf.variable_scope('block{}'.format(i)):
                block = _residual_block(block, filters, initializer, training)
                
    # Batch Normalization and Rectified Linear Unit layers. 
    bn_final = tf.keras.layers.BatchNormalization(
        momentum=BN_MOMENTUM, 
        fused=True, 
        scale=False)(block, training=training)
    relu_final = tf.nn.relu(bn_final)

    # Global Average Pooling and Classifier. 
    global_average_pool = tf.keras.layers.AveragePooling2D(
        pool_size=(8, 8),
        strides=1, 
        name='global_average_pooling')(relu_final)
    global_average_flat = tf.reshape(
        global_average_pool, 
        [-1, filters])
    
    '''K = kernel(keys, global_average_flat)
    KV =  tf.matmul(tf.transpose(K), V)
    KV_ = tf.diag(tf.reshape(tf.reciprocal( tf.matmul(KV,tf.ones((n_output,1)))) , [-1]))
    logits = tf.matmul(KV_, KV)'''

    logits = tf.keras.layers.Dense(
        units=64, 
        kernel_initializer = initializer)(global_average_flat)
    
    return logits
      
def euclidean_distance(a, b):
    # a.shape = N x D
    # b.shape = M x D
    N, D = tf.shape(a)[0], tf.shape(a)[1]
    M = tf.shape(b)[0]
    a = tf.tile(tf.expand_dims(a, axis=1), (1, M, 1))
    b = tf.tile(tf.expand_dims(b, axis=0), (N, 1, 1))
    return tf.reduce_mean(tf.square(a - b), axis=2)

"""## Config Setting"""

values = {
    "embedding_dim": [64],
    "n_way": [10],
    "n_shot": [200],
    "n_query": [100]
}

cross_combinations = ParameterGrid(values)

"""## Md"""

train_dataset = x_train;print(train_dataset.shape)
n_examples=train_dataset.shape[1];print(n_examples)
test_dataset=x_test;print(test_dataset.shape)
n_examples_test = test_dataset.shape[1];print(n_examples_test)

"""## Training"""

filename = '4. proto_cifar10_50_resnet_graph.pkl'
lr = 0.001

n_epochs = 801
n_episodes = 100
n_way = 10
n_shot = 200
n_query = 100
im_width, im_height, channels = 32, 32, 3
h_dim = 64
z_dim = 100
embedding_dim=64

n_classes = 10

n_test_epochs=5
n_test_episodes = 100
n_test_way = 10
n_test_shot = 200
n_test_query = 100

log_performance = []
convnet_activate = True

for indx, hyper_parameters in enumerate(cross_combinations):
  print('---------------------------------------------------------------------------')
  print('Combination '+str(indx+1)+' of '+str(len(cross_combinations))+' combinations ...')
  train_log = []
  test_log = []
  comb_log = {}
  # Hyperparameters
  embedding_dim = hyper_parameters['embedding_dim']
  n_way = n_test_way = hyper_parameters['n_way']
  n_shot = n_test_shot = hyper_parameters['n_shot']
  n_query = n_test_query = hyper_parameters['n_query']
  
  h_dim, z_dim = embedding_dim, embedding_dim

  print("hyper_parameters: ",hyper_parameters)
  # SetUp Model ------------------------------------------------------------------------------------------------------
  # ---------------------------------------------------------------------------------------------------------------------
  tf.reset_default_graph()
  x = tf.placeholder(tf.float32, [None, None, im_height, im_width, channels])
  q = tf.placeholder(tf.float32, [None, None, im_height, im_width, channels])
  x_shape = tf.shape(x)
  q_shape = tf.shape(q)
  num_classes, num_support = x_shape[0], x_shape[1]
  num_queries = q_shape[1]
  y = tf.placeholder(tf.int64, [None, None])
  y_one_hot = tf.one_hot(y, depth=num_classes)
  
  training_placeholder = tf.placeholder_with_default(False, ())

  emb_x = inference(tf.reshape(x, [num_classes * num_support, im_height, im_width, channels]), training_placeholder)
  emb_dim = tf.shape(emb_x)[-1]

  emb_x = tf.reduce_mean(tf.reshape(emb_x, [num_classes, num_support, emb_dim]), axis=1)

  training_placeholder2 = tf.placeholder_with_default(False, ()) 
  emb_q = inference2(tf.reshape(q, [num_classes * num_queries, im_height, im_width, channels]), training_placeholder2)

  dists = euclidean_distance(emb_q, emb_x)
  log_p_y = tf.reshape(tf.nn.log_softmax(-dists), [num_classes, num_queries, -1])
  ce_loss = -tf.reduce_mean(tf.reshape(tf.reduce_sum(tf.multiply(y_one_hot, log_p_y), axis=-1), [-1]))
  acc = tf.reduce_mean(tf.to_float(tf.equal(tf.argmax(log_p_y, axis=-1), y)))


  '''global_step = tf.train.get_or_create_global_step()
  lr = tf.train.piecewise_constant(tf.cast(global_step, tf.int32),
                                         LR_BOUNDARIES,
                                         LR_VALUES)

  # Optimizer
  train_op = tf.train.MomentumOptimizer(lr,  momentum=MOMENTUM, use_nesterov=True).minimize(ce_loss)'''
  train_op = tf.train.AdamOptimizer().minimize(ce_loss)
  # PUT THIS IN YOUR GRAPH
  var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)#, scope='head')
  gradios_amigos = tf.train.AdamOptimizer().compute_gradients(ce_loss, var_list=var_list)
  mi_gradios_amigos = [g for g in gradios_amigos if g[0] is not None]

  # Session
  sess = tf.InteractiveSession()
  init_op = tf.global_variables_initializer()
  sess.run(init_op)

  # Training Model ------------------------------------------------------------------------------------------------------
  # ---------------------------------------------------------------------------------------------------------------------
  stopping_criteria = 0
  last_test_acc = -1
  avg_acc = 0.
  avg_ls = 0.
  acc_train = []
  lss_train = []
  gradient_list_train = []
  acc_test = []
  lss_test = []
  gradient_list_test = []

  for ep in range(n_epochs):
      print('------------------------------------------------------------')
      print('EPOCH No.',ep)
      L2_tmp = []
      for epi in range(n_episodes):
          epi_classes = np.random.permutation(n_classes)[:n_way]
          support = np.zeros([n_way, n_shot, im_height, im_width,3], dtype=np.float32)
          query = np.zeros([n_way, n_query, im_height, im_width,3], dtype=np.float32)
          for i, epi_cls in enumerate(epi_classes):
              selected = np.random.permutation(n_examples)[:n_shot + n_query]
              support[i] = train_dataset[epi_cls, selected[:n_shot]]
              query[i] = train_dataset[epi_cls, selected[n_shot:]]
          #support = np.expand_dims(support, axis=-1)
          #query = np.expand_dims(query, axis=-1)
          labels = np.tile(np.arange(n_way)[:, np.newaxis], (1, n_query)).astype(np.uint8)

          _, ls, ac , grads = sess.run([train_op, ce_loss, acc, mi_gradios_amigos], feed_dict={x: support, q: query, y:labels})
          avg_acc += ac
          avg_ls += ls
          # compute L2 of this minibatch and append to list         
          L_dos = np.linalg.norm(np.hstack([g[0].flatten() for g in grads]), ord=2)
          L2_tmp.append(L_dos)

          if (epi+1) % 50 == 0:
              print('[epoch {}/{}, episode {}/{}] => loss: {:.5f}, acc: {:.5f}'.format(ep+1, n_epochs, epi+1, n_episodes, ls, ac))
      
      gradient_list_train.append(np.mean(L2_tmp))
      avg_acc /= (n_episodes)
      avg_ls /= (n_episodes)

      acc_train.append(avg_acc)
      lss_train.append(avg_ls)
   
      print('Average Train Accuracy: {:.5f}'.format(avg_acc))
      print('Average Train Loss: {:.5f}'.format(avg_ls))

      print('L2-norm of gradients in epochs {}:'.format(ep))
      print(np.mean(L2_tmp))

      train_log = (acc_train,lss_train,gradient_list_train)

      print('Testing....')
      avg_acc_test = 0.
      avg_ls_test = 0.
      L2_tmp_test = []

      if ep % 10 == 0 and ep > 2:#20-10
        for ep in range(n_test_epochs):
          
          for epi in range(n_test_episodes):
              epi_classes = np.random.permutation(n_test_classes)[:n_test_way]
              support = np.zeros([n_test_way, n_test_shot, im_height, im_width,3], dtype=np.float32)
              query = np.zeros([n_test_way, n_test_query, im_height, im_width,3], dtype=np.float32)
              for i, epi_cls in enumerate(epi_classes):
                  selected = np.random.permutation(n_examples_test)[:n_test_shot + n_test_query]
                  support[i] = test_dataset[epi_cls, selected[:n_test_shot]]
                  query[i] = test_dataset[epi_cls, selected[n_test_shot:]]
              #support = np.expand_dims(support, axis=-1)
              #query = np.expand_dims(query, axis=-1)
              labels = np.tile(np.arange(n_test_way)[:, np.newaxis], (1, n_test_query)).astype(np.uint8)
              ls, ac , grads= sess.run([ce_loss, acc, mi_gradios_amigos], feed_dict={x: support, q: query, y:labels})
              avg_acc_test += ac
              avg_ls_test += ls

              # compute L2 of this minibatch and append to list         
              L_dos = np.linalg.norm(np.hstack([g[0].flatten() for g in grads]), ord=2)
              L2_tmp_test.append(L_dos)

              if (epi+1) % 50 == 0:
                  print('[test episode {}/{}] => loss: {:.5f}, acc: {:.5f}'.format(epi+1, n_test_episodes, ls, ac))

        gradient_list_test.append(np.mean(L2_tmp_test))
        avg_acc_test /= (n_test_episodes * n_test_epochs)
        avg_ls_test /= (n_test_episodes* n_test_epochs)

        acc_test.append(avg_acc_test)
        lss_test.append(avg_ls_test)
    
        print('Average Test Accuracy: {:.5f}'.format(avg_acc_test))
        print('Average Test Loss: {:.5f}'.format(avg_ls_test))

        print('L2-norm of gradients in Test epochs {}:'.format(ep))
        print(np.mean(L2_tmp_test))
        
        test_log = (acc_test,lss_test, gradient_list_test)


        comb_log['train'] = train_log
        comb_log['test'] = test_log
        comb_log['params'] = hyper_parameters
        log_performance = comb_log

              # Stopping Criteria
        if avg_acc_test <= (last_test_acc * 1.005):
            print('- - - Model Not Improved - - -')
            stopping_criteria += 1
        else:
            print('- - - Model Improved - - -')
            stopping_criteria = 0
            last_test_acc = avg_acc_test

        if stopping_criteria >= 5:
            print('- - - Finished Learning: Stopping Criteria - - -')
            break
    
pickle_out = open(filename,"wb")
pickle.dump(log_performance, pickle_out)
pickle_out.close()

