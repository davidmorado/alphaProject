# -*- coding: utf-8 -*-
"""Varkey_Resnet++.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MqCrYz39KhbGSbA_RCmkuJ5GptwU87Qs
"""

from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
import tensorflow.keras as keras
from tensorflow.keras import models
from tensorflow.keras import layers
from tensorflow.keras import optimizers
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Layer
from tensorflow.keras.models import Model

class Varkeys(Layer):

    def __init__(self, keysize, dict_size, values, categories, **kwargs):
        self.output_dim = keysize
        self.initializer = keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=None)
        #self.initializer = keras.initializers.random_uniform([dict_size, keysize],maxval=1)
        self.values = tf.constant(values, dtype=tf.float32, shape = (n_keys, n_output))
        self.categories = categories
        self.keysize = keysize 
        self.dict_size = dict_size
        super(Varkeys, self).__init__(**kwargs)

    def build(self, input_shape):
        # Create a trainable weight variable for this layer.
        self.keys = self.add_weight(name='keys', 
                                      shape=(self.dict_size, self.keysize),
                                      initializer=self.initializer,
                                      trainable=True)
        
        super(Varkeys, self).build(input_shape)  # Be sure to call this at the end

    def call(self, x):
        KV =  tf.matmul(tf.transpose(self.kernel(self.keys, x)), self.values)
        KV_ = tf.diag(tf.reshape(tf.reciprocal( tf.matmul(KV,tf.ones((self.categories,1)))) , [-1]))
        output = tf.matmul(KV_, KV)
        return output

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.categories)

    
    def sq_distance(self, A, B):
        print('im in distance function')
        row_norms_A = tf.reduce_sum(tf.square(A), axis=1)
        row_norms_A = tf.reshape(row_norms_A, [-1, 1])  # Column vector.

        row_norms_B = tf.reduce_sum(tf.square(B), axis=1)
        row_norms_B = tf.reshape(row_norms_B, [1, -1])  # Row vector.

        return row_norms_A - 2 * tf.matmul(A, tf.transpose(B)) + row_norms_B


    def kernel (self, A,B):
        print('im in kernel function!!')
        d = self.sq_distance(A,B)
        o = tf.reciprocal(d+1)
        #o = tf.exp(-d/10)
        return o
    
    def kernel_cos(self, A,B):
      
        normalize_A = tf.nn.l2_normalize(A,1)        
        normalize_B = tf.nn.l2_normalize(B,1)
        cossim = tf.matmul(normalize_B, tf.transpose(normalize_A))
        return tf.transpose(cossim)
      
    def kernel_gauss(self, A,B):

        normalize_A = tf.nn.l2_normalize(A,1)        
        normalize_B = tf.nn.l2_normalize(B,1)
        d = self.sq_distance(A,B)
        o = tf.exp(-(d)/100)
        return o


(x_train, y_train), (x_test, y_test) = cifar10.load_data()
input_shape = x_train.shape[1:]
num_classes = np.max(y_test)+1
num_samples = x_train.shape[0]

x_train = x_train/255
x_test = x_test/255
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

n_output = 10
embedding_dim = 64
n_keys_per_class = 1
values = np.vstack((np.repeat([[1,0,0,0,0,0,0,0,0,0]], n_keys_per_class, axis=0),
                    np.repeat([[0,1,0,0,0,0,0,0,0,0]], n_keys_per_class, axis=0),
                    np.repeat([[0,0,1,0,0,0,0,0,0,0]], n_keys_per_class, axis=0),
                    np.repeat([[0,0,0,1,0,0,0,0,0,0]], n_keys_per_class, axis=0),
                    np.repeat([[0,0,0,0,1,0,0,0,0,0]], n_keys_per_class, axis=0),
                    np.repeat([[0,0,0,0,0,1,0,0,0,0]], n_keys_per_class, axis=0),
                    np.repeat([[0,0,0,0,0,0,1,0,0,0]], n_keys_per_class, axis=0),
                    np.repeat([[0,0,0,0,0,0,0,1,0,0]], n_keys_per_class, axis=0),
                    np.repeat([[0,0,0,0,0,0,0,0,1,0]], n_keys_per_class, axis=0),
                    np.repeat([[0,0,0,0,0,0,0,0,0,1]], n_keys_per_class, axis=0)))
n_keys= values.shape[0]
V = tf.constant(values, dtype=tf.float32, shape = (n_keys, n_output))
batch_size = 32
lr = 0.0001
epochs = 10
sigma=0.01


def custom_loss(layer, sigma=0.01, custom=1):

    # Create a loss function that adds the MSE loss to the mean of all squared activations of a specific layer
    def loss(y_true,y_pred):

      if(custom==1):
        return keras.losses.categorical_crossentropy(y_true=y_true, y_pred=y_pred)+sigma*tf.reduce_sum(layer.kernel(layer.keys, layer.keys))# + sigma*tf.reduce_mean(layer.kernel(layer.keys, layer.keys) , axis=-1)
      else:
        return keras.losses.categorical_crossentropy(y_true=y_true, y_pred=y_pred)
   
    # Return a function
    return loss


def construct_models ():

  conv_base = ResNet50(weights='imagenet', include_top=False, input_shape=(200, 200, 3))
  input = layers.Input(shape=( 32,32,3,))
  x=layers.UpSampling2D((2,2) )(input)
  x=layers.UpSampling2D((2,2))(x)
  x=layers.UpSampling2D((2,2))(x)
  x=conv_base(x)
  x=layers.Flatten()(x)
  x=layers.BatchNormalization()(x)
  x=layers.Dense(128, activation='relu')(x)
  x=layers.Dropout(0.5)(x)
  x=layers.BatchNormalization()(x)
  x=layers.Dense(64, activation='relu')(x)
  x=layers.BatchNormalization()(x)

  varkeys_output = Varkeys(embedding_dim, n_keys, values, num_classes)(x)
  plain_output = layers.Activation('softmax')(layers.Dense(num_classes)(x))

  return input, varkeys_output, plain_output

from tensorflow.keras.callbacks import EarlyStopping

for train_pct in [0.75]:

  print("Train_pct=", train_pct)
  n_train = x_train.shape[0]
  idx = np.arange(n_train)
  np.random.shuffle(idx)

  train_samples = int(train_pct*n_train)
  x_train_pct = x_train[idx][:train_samples]
  y_train_pct = y_train[idx][:train_samples]


  input, varkeys_output, plain_output = construct_models()
  plain_model = Model(inputs=input, outputs=plain_output)
  varkeys_model = Model(inputs=input, outputs=varkeys_output)

  varkeys_model.compile(loss=custom_loss(varkeys_model.layers[-1], sigma, 1),#keras.losses.categorical_crossentropy,
              # optimizer=keras.optimizers.SGD(lr=0.1),
              optimizer = optimizers.RMSprop(lr=2e-4),
              metrics=['accuracy'])

  plain_model.compile(loss= keras.losses.categorical_crossentropy,#keras.losses.categorical_crossentropy,
              # optimizer=keras.optimizers.SGD(lr=0.1),
              #optimizer = keras.optimizers.rmsprop(lr=lr, decay=1e-6),
              optimizer = optimizers.RMSprop(lr=2e-4),
              metrics=['accuracy'])


  callbacks = [EarlyStopping(monitor='val_loss', patience=5)]
  plain_model.fit(x_train_pct, y_train_pct,
          batch_size=  batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test),
          callbacks = callbacks)

  varkeys_model.fit(x_train_pct, y_train_pct,
          batch_size=  batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test),
          callbacks = callbacks)


  scores = varkeys_model.evaluate(x_test, y_test)
  print("Final acc: \n%s: %.2f%%" % (varkeys_model.metrics_names[1], scores[1]*100))

