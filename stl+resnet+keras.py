# -*- coding: utf-8 -*-
"""STL+Resnet+Keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14dhQelNrVd2SpvtHtGg2TeIk_iluCEM7
"""

from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
import tensorflow.keras as keras
from tensorflow.keras import models
from tensorflow.keras import layers
from tensorflow.keras import optimizers
import tensorflow as tf
from keras.utils import np_utils
from keras.models import load_model
from keras.datasets import cifar10
from keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Layer
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from keras.layers import Activation
from sklearn.utils import shuffle
import pickle

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

x_train = x_train / 255.0
x_test = x_test / 255.0

y_train = np_utils.to_categorical(y_train, 10)
y_test = np_utils.to_categorical(y_test, 10)

print(x_train.shape)
print(x_test.shape)

class Varkeys(Layer):

    def __init__(self, keysize, dict_size, values, categories, **kwargs):
        self.output_dim = keysize
        self.initializer = keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=None)
        #self.initializer = keras.initializers.random_uniform([dict_size, keysize],maxval=1)
        self.values =  tf.constant(values, dtype=tf.float32, shape = (n_keys, num_classes))
        self.categories = categories
        self.keysize = keysize 
        self.dict_size = dict_size
        super(Varkeys, self).__init__(**kwargs)

    def build(self, input_shape):
        # Create a trainable weight variable for this layer.
        self.keys = self.add_weight(name='keys', 
                                      shape=(self.dict_size, self.keysize),
                                      initializer=self.initializer,
                                      trainable=True)
        
        super(Varkeys, self).build(input_shape)  # Be sure to call this at the end

    def call(self, x):
        KV =  tf.matmul(tf.transpose(self.kernel(self.keys, x)), self.values)
        KV_ = tf.diag(tf.reshape(tf.reciprocal( tf.matmul(KV,tf.ones((self.categories,1)))) , [-1]))
        output = tf.matmul(KV_, KV)
        return output

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.categories)

    
    def sq_distance(self, A, B):
        print('im in distance function')
        row_norms_A = tf.reduce_sum(tf.square(A), axis=1)
        row_norms_A = tf.reshape(row_norms_A, [-1, 1])  # Column vector.

        row_norms_B = tf.reduce_sum(tf.square(B), axis=1)
        row_norms_B = tf.reshape(row_norms_B, [1, -1])  # Row vector.

        return row_norms_A - 2 * tf.matmul(A, tf.transpose(B)) + row_norms_B


    def kernel (self, A,B):
        print('im in kernel function!!')
        d = self.sq_distance(A,B)
        o = tf.reciprocal(d+1)
        #o = tf.exp(-d/10)
        return o
    
    def kernel_cos(self, A,B):
      
        normalize_A = tf.nn.l2_normalize(A,1)        
        normalize_B = tf.nn.l2_normalize(B,1)
        cossim = tf.matmul(normalize_B, tf.transpose(normalize_A))
        return tf.transpose(cossim)
      
    def kernel_gauss(self, A,B):
        d = self.sq_distance(A,B)
        o = tf.exp(-(d)/100)
        return o

    def set_values (self, values):
        self.values =  tf.constant(values, dtype=tf.float32, shape = (n_keys, num_classes))

class SoftTL(Layer):

    def __init__(self, emb_size, n_centers, n_classes, gamma=0.1, **kwargs):
        self.initializer = keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=None)
        self.emb_size = emb_size
        self.gamma = gamma
        self.n_centers = n_centers
        self.n_classes = n_classes
        super(SoftTL, self).__init__(**kwargs)

    def build(self, input_shape):
        self.keys = self.add_weight(name='keys', 
                                      shape=(self.n_centers, self.emb_size, self.n_classes),
                                      initializer=self.initializer,
                                      trainable=True)     
        super(SoftTL, self).build(input_shape)

    def call(self, pInput):
        inner_logits = tf.einsum('ie,kec->ikc', pInput, self.keys)
        inner_SoftMax = tf.nn.softmax((1/self.gamma)*inner_logits, axis=1)
        output = tf.reduce_sum( tf.multiply(inner_SoftMax, inner_logits), axis=1)
        return output

USING_RESNET = True

# Training Set-Up
data_percentages = [1.0]

embedding_dim     = 64
n_keys_per_class  = 1
n_keys            = 10
num_classes       = 10

total_epochs = 100


# Soft Triple Loss Function
def custom_loss(layer, lamb=5, delta=0.01):

    def loss(y_true, y_pred):
      s = lamb*(y_pred - delta*y_true)
      outer_SoftMax = tf.nn.softmax(s)
      soft_triple_loss = -tf.reduce_sum(tf.log(tf.reduce_sum(tf.multiply(outer_SoftMax, y_true), axis=1)))
      return soft_triple_loss
    return loss


# Training ...
#model.compile(optimizer=optimizers.RMSprop(lr=0.0001), loss=custom_loss(model.layers[-2]), metrics=['acc'])
historical_log = {}
GDs = []
for current_percentage in data_percentages:

  if USING_RESNET:
    # Network Architecture 
    conv_base = ResNet50(weights='imagenet', include_top=False, input_shape=(200, 200, 3))

    model = models.Sequential()
    model.add(layers.UpSampling2D((2,2) ))
    model.add(layers.UpSampling2D((2,2)))
    model.add(layers.UpSampling2D((2,2)))
    model.add(conv_base)

    model.add(layers.Flatten())
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.Dropout(0.5))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.BatchNormalization())

    model.add(SoftTL(embedding_dim, n_keys_per_class, num_classes))
    model.add(layers.Softmax())
  else:
    embedding_dim     = 100
    model = CNN_keys(layers_size=[32, 64, 512], embedding_dim = embedding_dim, num_classes=10)

  model.compile(optimizer=optimizers.RMSprop(lr=0.00002), loss=custom_loss(model.layers[-2]), metrics=['acc'])


  print('Training with',current_percentage*100, '%')
  callbacks = EarlyStopping(monitor='val_loss', patience=5)

  limit_train = int(current_percentage*x_train.shape[0])
  x_train,  y_train = shuffle(x_train,  y_train,    random_state=0)
  current_x_train, current_y_train = x_train[:limit_train],   y_train[:limit_train]
  
  history = model.fit(current_x_train, current_y_train, 
                      epochs=total_epochs, batch_size=32, validation_data=(x_test, y_test), 
                      callbacks=[callbacks])
  
  current_scores  = history.history
  historical_log[current_percentage] = current_scores

  file_name = 'STL_Convnet_'+str(int(current_percentage*100))+'.pkl'
  pickle.dump( current_scores, open( file_name, "wb" ) )
  save_to_drive(file_name)

