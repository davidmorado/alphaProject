import pickle
import numpy as np
from keras.datasets import cifar10
from keras.utils import to_categorical
from CNN import CNN
from fit_evaluate2 import fit_evaluate
import sys
import os

# creates folders
folders = ['models', 'gridresults', 'tb_logs', 'errs', 'logs']
for f in folders:
    try:
        os.makedirs(f)
    except OSError:
        pass

# Data Loading and Preprocessing
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
input_shape = x_train.shape[1:]
num_classes = np.max(y_test)+1
num_samples = x_train.shape[0]

x_train = x_train/255
x_test = x_test/255
y_train = to_categorical(y_train, num_classes)
y_test = to_categorical(y_test, num_classes)

# Hyperparameters:
# number of target categories
num_categories = 10
# batch size to train SGD
batch_size = 64
# number of epochs to train the network for
#epochs = 500
epochs = 1

# Hyperparameters:
hp_dict = {
    'embedding_dim': int(sys.argv[1]),
    'train_percentage': float(sys.argv[2]),
    'learning_rate': float(sys.argv[3]),
    # theta HP
    'theta': float(sys.argv[4]),
    # fraction of data in memory
    'cache_fraction': float(sys.argv[5]),
    # trade-off cache V model for prediction
    'lmbd': float(sys.argv[6])
}

# size of the embedding generated by CNN
embedding_dim = hp_dict['embedding_dim']
# Training percentage (change to check if code even runs)
train_percentage = hp_dict['train_percentage']
# learning rate
learning_rate = hp_dict['learning_rate']

print("Percentage of training =", train_percentage)
idx = np.random.choice(num_samples, int(train_percentage*num_samples))
x_train_ = x_train[idx,]
y_train_ = y_train[idx,]

model = CNN(
    num_categories,
    input_shape=input_shape, 
    layers=[32, 64, 512], 
    embedding_dim=embedding_dim)

modelpath = F"Adam_es={embedding_dim}_tp={train_percentage}_df={hp_dict['cache_fraction']}_lr={learning_rate}_lmbd={hp_dict['lmbd']}"

metrics_dict, key = fit_evaluate(model, x_train_, y_train_, x_test, y_test, batch_size, epochs, 
                                lr=learning_rate, logstring=F'tb_logs/{modelpath}',
                                theta=hp_dict['theta'],
                                cache_fraction=hp_dict['cache_fraction'], lmbd=hp_dict['lmbd'])

# print('eval in main')
# mem_loss, mem_accuracy = model.evaluate(x_test, y_test, verbose=1)
# print('model loss:', mem_loss)
# print('model accuracy:', mem_accuracy)

# causes OSError: Unable to create file (unable to lock file, errno = 37, error message = 'No locks available')
# model.save(F'models/' + modelpath)

out_results = (hp_dict, metrics_dict, key)
filename = F"gridresults/{modelpath}.pkl"
with open(filename, 'wb') as f:
  pickle.dump(out_results, f)
