import pickle
import numpy as np
from keras.datasets import cifar10
from keras.utils import to_categorical
from CNN_VK import CNN_VK
from fit_evaluate import fit_evaluate   

# Data Loading and Preprocessing 
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
input_shape = x_train.shape[1:]
num_classes = np.max(y_test)+1
num_samples = x_train.shape[0]

x_train = x_train/255
x_test = x_test/255
y_train = to_categorical(y_train, num_classes)
y_test = to_categorical(y_test, num_classes)
    
# Hyperparameters:
# number of target categories
num_categories = 10
# size of the embedding generated by CNN_VK
embedding_dim = 20
# batch size to train SGD
batch_size = 64
# learning rate
lr = 0.0001
# bandwidth parameter for kernel regression
bandwidth = 10000
# number of epochs to train the network for
epochs = 500
# Determines how many keys there are per target category (n_categories).
# This should be varied in this script to evaluate the effect of memory 
# size on network performance.
keys_per_class_grid = range(10, 1001, 10)
keys_per_class_grid = 20
# Training percentage (change to check if code even runs)
p = 1.0
#p = 0.1

for n_keys_per_class in keys_per_class_grid:

    print("Percentage of training =", p)
    idx = np.random.choice(num_samples, int(p*num_samples))
    x_train_ = x_train[idx,]
    y_train_ = y_train[idx,]

    print("CNN+Keys...")
    print("CNN with " + str(n_keys_per_class) + " keys per class.")
    model = CNN_VK(
        num_categories,
        input_shape=input_shape, 
        layers=[32, 64, 512], 
        #num_categories=num_categories, 
        embedding_dim=embedding_dim, 
        n_keys_per_class=n_keys_per_class, 
        bandwidth=bandwidth)
    results = fit_evaluate(model, x_train_, y_train_, x_test, y_test, batch_size, epochs, lr)
    
    filename = "results/results1/CNN_" + str(n_keys_per_class) + "_keys.pkl"
    
    with open(filename, 'wb') as f:
      pickle.dump(results, f)
